Training mode
Raw tokens: 22315  (1000 sentences)
Token-supervision tokens: 22315  (1000 sentences)
Type-supervision TD-entries: 4238  (4022 word types)
tsmooth: AddLambdaTransitionDistributioner(0.1)
esmooth: AddLambdaEmissionDistributioner(0.1)
emTrainer: SoftEmHmmTaggerTrainer(6, UnsmoothedTransitionDistributioner(), UnsmoothedEmissionDistributioner(), alphaT=0.000000, alphaE=0.000000)
Induce a soft tagging of the raw data
Extract a generalized tag dictionary
Induce a hard tagging via model minimization on the soft LP output
learn a smoothed HMM from EM
learn an HMM initialized with the estimated transition and emission distributions
raw tokens = 22315  (1000 sentences)
numWords = 4022
numTags  = 17

Make Indexed Distributions
Make Prior Counts (from the 1000 gold labeled sentences)
Start Training
iteration 1:   0.042 sec   avgLogProb=-141.65266567497346,   avgProb=3.0271152116825445E-62
iteration 2:   0.036 sec   avgLogProb=-137.4445024344209,    avgProb=2.035218420796938E-60
iteration 3:   0.031 sec   avgLogProb=-137.1255251343326,    avgProb=2.7998908808874395E-60
iteration 4:   0.024 sec   avgLogProb=-137.05360010966623,   avgProb=3.0086921117897623E-60
iteration 5:   0.025 sec   avgLogProb=-137.04350312865554,   avgProb=3.039224703009508E-60
iteration 6:   0.026 sec   avgLogProb=-137.04477677708672,   avgProb=3.0353562632735583E-60
MAX ITERATIONS REACHED
Indexing events using cutoff of 10

	Computing event counts...  done. 22315 events
	Indexing...  done.
Sorting and merging events... done. Reduced 22315 events to 19919.
Done indexing.
Incorporating indexed data for training...  
done.
	Number of Event Tokens: 19919
	    Number of Outcomes: 15
	  Number of Predicates: 3027
...done.
Computing model parameters ...
Performing 100 iterations.
  1:  ... loglikelihood=-60430.140237586056	0.2614833071924714
  2:  ... loglikelihood=-29461.07798988379	0.8625140040331616
  3:  ... loglikelihood=-20196.525248732694	0.8846963925610576
  4:  ... loglikelihood=-15896.925411699378	0.8972440062738068
  5:  ... loglikelihood=-13385.625861669876	0.9086264844275151
  6:  ... loglikelihood=-11723.038084411193	0.9151243558144746
  7:  ... loglikelihood=-10530.667476200746	0.921622227201434
  8:  ... loglikelihood=-9626.53069737219	0.926417208155949
  9:  ... loglikelihood=-8912.456881808223	0.9300470535514228
 10:  ... loglikelihood=-8330.918700832803	0.9337217118530137
 11:  ... loglikelihood=-7845.994546571663	0.9360071700649787
 12:  ... loglikelihood=-7434.018057852414	0.9387407573381134
 13:  ... loglikelihood=-7078.640202082147	0.9406228993950257
 14:  ... loglikelihood=-6768.150026153061	0.942729105982523
 15:  ... loglikelihood=-6493.9334596116305	0.9447456867577863
 16:  ... loglikelihood=-6249.508993756646	0.9459556352229442
 17:  ... loglikelihood=-6029.899693726756	0.9477033385615057
 18:  ... loglikelihood=-5831.215155034825	0.9491373515572484
 19:  ... loglikelihood=-5650.366218708461	0.9510643065202778
 20:  ... loglikelihood=-5484.865433445355	0.9525431324221376
 21:  ... loglikelihood=-5332.6844650215235	0.9536186421689447
 22:  ... loglikelihood=-5192.150336507759	0.9546493390096348
 23:  ... loglikelihood=-5061.868767207324	0.9559937261931436
 24:  ... loglikelihood=-4940.666796561325	0.956800358503249
 25:  ... loglikelihood=-4827.549333187473	0.9579654940622899
 26:  ... loglikelihood=-4721.665852218596	0.9591306296213309
 27:  ... loglikelihood=-4622.28453862064	0.9600717006497871
 28:  ... loglikelihood=-4528.771940750464	0.9609679587721264
 29:  ... loglikelihood=-4440.576754358469	0.9617745910822317
 30:  ... loglikelihood=-4357.216753420891	0.9627604750168048
 31:  ... loglikelihood=-4278.268158808341	0.9635222944207932
 32:  ... loglikelihood=-4203.356923243607	0.9643289267308985
 33:  ... loglikelihood=-4132.151539817997	0.965001120322653
 34:  ... loglikelihood=-4064.3570722167406	0.9655388751960564
 35:  ... loglikelihood=-3999.71017117408	0.966121442975577
 36:  ... loglikelihood=-3937.974891667492	0.9667040107550975
 37:  ... loglikelihood=-3878.9391638221464	0.9673313914407349
 38:  ... loglikelihood=-3822.411800428985	0.9676898946896706
 39:  ... loglikelihood=-3768.219947399384	0.9680932108447233
 40:  ... loglikelihood=-3716.206901845153	0.9684517140936589
 41:  ... loglikelihood=-3666.2302369080426	0.9689894689670625
 42:  ... loglikelihood=-3618.160183846753	0.9693479722159982
 43:  ... loglikelihood=-3571.8782309039775	0.9696616625588169
 44:  ... loglikelihood=-3527.275905661245	0.9701546045261035
 45:  ... loglikelihood=-3484.253713346339	0.9706027335872731
 46:  ... loglikelihood=-3442.7202082046797	0.9708716110239749
 47:  ... loglikelihood=-3402.5911788195417	0.9714541788034954
 48:  ... loglikelihood=-3363.788931346653	0.971991933676899
 49:  ... loglikelihood=-3326.241657159601	0.9724400627380686
 50:  ... loglikelihood=-3289.882873491791	0.9727089401747704
 51:  ... loglikelihood=-3254.650927389961	0.9730226305175891
 52:  ... loglikelihood=-3220.4885547380877	0.9734707595787587
 53:  ... loglikelihood=-3187.342487312989	0.9737396370154604
 54:  ... loglikelihood=-3155.1631018443204	0.9740085144521622
 55:  ... loglikelihood=-3123.9041059013093	0.9745014564194489
 56:  ... loglikelihood=-3093.5222561461433	0.9747255209500336
 57:  ... loglikelihood=-3063.9771051027424	0.9752632758234372
 58:  ... loglikelihood=-3035.230773104895	0.9756217790723728
 59:  ... loglikelihood=-3007.2477425279767	0.9757114048846067
 60:  ... loglikelihood=-2979.994671784793	0.9758906565090746
 61:  ... loglikelihood=-2953.440226886103	0.9760699081335424
 62:  ... loglikelihood=-2927.5549286449077	0.9762939726641273
 63:  ... loglikelihood=-2902.3110138395773	0.9765180371947121
 64:  ... loglikelihood=-2877.6823088561564	0.9769661662558817
 65:  ... loglikelihood=-2853.6441145091017	0.9772350436925835
 66:  ... loglikelihood=-2830.1731008914044	0.9775487340354022
 67:  ... loglikelihood=-2807.247211240397	0.977683172753753
 68:  ... loglikelihood=-2784.845573920584	0.9779072372843379
 69:  ... loglikelihood=-2762.9484217272147	0.9780864889088058
 70:  ... loglikelihood=-2741.5370178023636	0.9782657405332735
 71:  ... loglikelihood=-2720.593587533528	0.9784001792516245
 72:  ... loglikelihood=-2700.101255872309	0.9786242437822092
 73:  ... loglikelihood=-2680.043989571543	0.9788034954066771
 74:  ... loglikelihood=-2660.406543891371	0.979027559937262
 75:  ... loglikelihood=-2641.174413371517	0.9791619986556128
 76:  ... loglikelihood=-2622.333786308826	0.9792964373739637
 77:  ... loglikelihood=-2603.87150261465	0.9793860631861976
 78:  ... loglikelihood=-2585.7750147601364	0.9794756889984315
 79:  ... loglikelihood=-2568.032351545252	0.9796101277167825
 80:  ... loglikelihood=-2550.6320844540473	0.9798790051534843
 81:  ... loglikelihood=-2533.563296380173	0.980103069684069
 82:  ... loglikelihood=-2516.815552528738	0.9803719471207708
 83:  ... loglikelihood=-2500.3788733169963	0.9804615729330047
 84:  ... loglikelihood=-2484.243709114386	0.9806856374635895
 85:  ... loglikelihood=-2468.4009166756637	0.9809097019941744
 86:  ... loglikelihood=-2452.8417371349187	0.9810441407125252
 87:  ... loglikelihood=-2437.5577754399483	0.9810889536186421
 88:  ... loglikelihood=-2422.5409811163154	0.9811337665247591
 89:  ... loglikelihood=-2407.7836302615206	0.981313018149227
 90:  ... loglikelihood=-2393.278308676519	0.9814922697736949
 91:  ... loglikelihood=-2379.017896051382	0.9816715213981627
 92:  ... loglikelihood=-2364.995551127699	0.9817163343042796
 93:  ... loglikelihood=-2351.2046977672517	0.9818059601165136
 94:  ... loglikelihood=-2337.639011862521	0.9819852117409814
 95:  ... loglikelihood=-2324.292409029128	0.9819403988348644
 96:  ... loglikelihood=-2311.159033026068	0.9820300246470983
 97:  ... loglikelihood=-2298.2332448529323	0.9822092762715662
 98:  ... loglikelihood=-2285.509612478264	0.9823437149899171
 99:  ... loglikelihood=-2272.98290115592	0.9826125924266189
100:  ... loglikelihood=-2260.648064290278	0.9827918440510867
Writing tagger model to /Users/aihb/experiment/pos/my-pos/model4github/low-resource-pos-tagging-2014/2hours/t1/train1.nopipe.pipe.ser

 Re-training finished for train1!!!
ls ./2hours/t1/
ctest1
ctest1.nopipe
ctest1.nopipe.pipe
ctest1.nopipe.word
otest.nopipe.word
train1
train1.nopipe
train1.nopipe.pipe
train1.nopipe.pipe.ser
train1.nopipe.word
Training mode
Raw tokens: 87310  (4000 sentences)
Token-supervision tokens: 87310  (4000 sentences)
Type-supervision TD-entries: 9585  (9039 word types)
tsmooth: AddLambdaTransitionDistributioner(0.1)
esmooth: AddLambdaEmissionDistributioner(0.1)
emTrainer: SoftEmHmmTaggerTrainer(6, UnsmoothedTransitionDistributioner(), UnsmoothedEmissionDistributioner(), alphaT=0.000000, alphaE=0.000000)
Induce a soft tagging of the raw data
Extract a generalized tag dictionary
Induce a hard tagging via model minimization on the soft LP output
learn a smoothed HMM from EM
learn an HMM initialized with the estimated transition and emission distributions
raw tokens = 87310  (4000 sentences)
numWords = 9039
numTags  = 17

Make Indexed Distributions
Make Prior Counts (from the 4000 gold labeled sentences)
Start Training
iteration 1:   0.137 sec   avgLogProb=-141.9728703272587,    avgProb=2.197686988384468E-62
iteration 2:   0.109 sec   avgLogProb=-138.52734250920813,   avgProb=6.891909334702808E-61
iteration 3:   0.104 sec   avgLogProb=-137.94220356517692,   avgProb=1.2372634113875307E-60
iteration 4:   0.107 sec   avgLogProb=-137.83013773683086,   avgProb=1.383986140162496E-60
iteration 5:   0.110 sec   avgLogProb=-137.81883130476663,   avgProb=1.3997228809872403E-60
iteration 6:   0.107 sec   avgLogProb=-137.82492281638073,   avgProb=1.3912223695685445E-60
MAX ITERATIONS REACHED
Indexing events using cutoff of 10

	Computing event counts...  done. 87310 events
	Indexing...  done.
Sorting and merging events... done. Reduced 87310 events to 77509.
Done indexing.
Incorporating indexed data for training...  
done.
	Number of Event Tokens: 77509
	    Number of Outcomes: 15
	  Number of Predicates: 8413
...done.
Computing model parameters ...
Performing 100 iterations.
  1:  ... loglikelihood=-236439.8630580941	0.2846752949261253
  2:  ... loglikelihood=-101631.72842622806	0.876508990951781
  3:  ... loglikelihood=-66360.26751309908	0.9030351620662009
  4:  ... loglikelihood=-50661.46534227043	0.9188409116939641
  5:  ... loglikelihood=-41788.16304850818	0.9290802886267323
  6:  ... loglikelihood=-36071.41227393316	0.936799908372466
  7:  ... loglikelihood=-32061.442073483748	0.9423662810674608
  8:  ... loglikelihood=-29076.99172220571	0.946936204329401
  9:  ... loglikelihood=-26757.630895088707	0.950624212575879
 10:  ... loglikelihood=-24894.962984202015	0.9529950750200435
 11:  ... loglikelihood=-23360.076617212333	0.9552628564883747
 12:  ... loglikelihood=-22068.934893923455	0.957118314053373
 13:  ... loglikelihood=-20964.389517182535	0.9586988890161493
 14:  ... loglikelihood=-20006.18961399143	0.9602336502118887
 15:  ... loglikelihood=-19165.130257268243	0.9614019012713321
 16:  ... loglikelihood=-18419.47381172058	0.9626503264230901
 17:  ... loglikelihood=-17752.68034581562	0.963658229297904
 18:  ... loglikelihood=-17151.922267146267	0.9647577597067919
 19:  ... loglikelihood=-16607.084520869197	0.9655480471881801
 20:  ... loglikelihood=-16110.074253252864	0.9666246707135494
 21:  ... loglikelihood=-15654.333018373061	0.9674722254037338
 22:  ... loglikelihood=-15234.484902759137	0.9682739663268812
 23:  ... loglikelihood=-14846.078005009267	0.9689726262741954
 24:  ... loglikelihood=-14485.391400295859	0.9696254724544726
 25:  ... loglikelihood=-14149.288904617073	0.9703470392853052
 26:  ... loglikelihood=-13835.106840769986	0.9709655251403047
 27:  ... loglikelihood=-13540.566874905722	0.9715954644370633
 28:  ... loglikelihood=-13263.707588832433	0.972179589966785
 29:  ... loglikelihood=-13002.830229345667	0.9727064482877105
 30:  ... loglikelihood=-12756.455310320272	0.9731645859580804
 31:  ... loglikelihood=-12523.287612834414	0.9736570839537281
 32:  ... loglikelihood=-12302.187748160995	0.9739892337647463
 33:  ... loglikelihood=-12092.148894444745	0.9744931852021532
 34:  ... loglikelihood=-11892.277642629428	0.9749627763142824
 35:  ... loglikelihood=-11701.778127028545	0.9752147520329859
 36:  ... loglikelihood=-11519.93879628089	0.9755354484022448
 37:  ... loglikelihood=-11346.12131849512	0.9759592257473371
 38:  ... loglikelihood=-11179.751221509552	0.9763830030924293
 39:  ... loglikelihood=-11020.309952796439	0.9766349788111327
 40:  ... loglikelihood=-10867.328108728889	0.9768984079715954
 41:  ... loglikelihood=-10720.379633584644	0.9772305577826137
 42:  ... loglikelihood=-10579.076827928393	0.9775168938265949
 43:  ... loglikelihood=-10443.066036510074	0.9777688695452984
 44:  ... loglikelihood=-10312.023909641528	0.9779635780552056
 45:  ... loglikelihood=-10185.654150793589	0.9782728209827053
 46:  ... loglikelihood=-10063.684678126438	0.9786049707937235
 47:  ... loglikelihood=-9945.865139723763	0.9789142137212232
 48:  ... loglikelihood=-9831.964732093258	0.9791547359981675
 49:  ... loglikelihood=-9721.77027953232	0.9793494445080747
 50:  ... loglikelihood=-9615.084538565321	0.9796472339938151
 51:  ... loglikelihood=-9511.724697148667	0.9798419425037224
 52:  ... loglikelihood=-9411.521042897071	0.9800481044553888
 53:  ... loglikelihood=-9314.315778395026	0.9801969991982591
 54:  ... loglikelihood=-9219.96196484654	0.9804375214752032
 55:  ... loglikelihood=-9128.32257800647	0.980655136868629
 56:  ... loglikelihood=-9039.269662582243	0.9808498453785363
 57:  ... loglikelihood=-8952.683573218605	0.9810101935631658
 58:  ... loglikelihood=-8868.45229179225	0.9811934486313137
 59:  ... loglikelihood=-8786.470812114216	0.9813652502577025
 60:  ... loglikelihood=-8706.640584317729	0.9815599587676097
 61:  ... loglikelihood=-8628.86901220568	0.9816744931852022
 62:  ... loglikelihood=-8553.068997690647	0.9818004810445539
 63:  ... loglikelihood=-8479.158527197189	0.9819493757874241
 64:  ... loglikelihood=-8407.06029552866	0.9821898980643683
 65:  ... loglikelihood=-8336.70136324864	0.9823158859237201
 66:  ... loglikelihood=-8268.012844100243	0.9825678616424236
 67:  ... loglikelihood=-8200.929619395607	0.9827167563852938
 68:  ... loglikelihood=-8135.390076662402	0.982911464895201
 69:  ... loglikelihood=-8071.33587014894	0.983003092429275
 70:  ... loglikelihood=-8008.711701053931	0.9830603596380713
 71:  ... loglikelihood=-7947.465115588044	0.9832207078227008
 72:  ... loglikelihood=-7887.546319179544	0.9834497766578857
 73:  ... loglikelihood=-7828.90800531801	0.9835757645172374
 74:  ... loglikelihood=-7771.505197691047	0.9836673920513114
 75:  ... loglikelihood=-7715.295104408252	0.9838391936777001
 76:  ... loglikelihood=-7660.236983233888	0.9839766349788112
 77:  ... loglikelihood=-7606.292016856594	0.9841713434887184
 78:  ... loglikelihood=-7553.4231973236	0.9842858779063108
 79:  ... loglikelihood=-7501.595218853391	0.9844118657656625
 80:  ... loglikelihood=-7450.774378318506	0.984480586416218
 81:  ... loglikelihood=-7400.928482754824	0.9845836673920513
 82:  ... loglikelihood=-7352.02676332053	0.9846982018096438
 83:  ... loglikelihood=-7304.039795179161	0.98475546901844
 84:  ... loglikelihood=-7256.939422826942	0.9848356431107548
 85:  ... loglikelihood=-7210.698690437705	0.985030351620662
 86:  ... loglikelihood=-7165.291776825087	0.985121979154736
 87:  ... loglikelihood=-7120.693934670069	0.9852365135723284
 88:  ... loglikelihood=-7076.8814336832465	0.9853395945481617
 89:  ... loglikelihood=-7033.831507404402	0.9854312220822357
 90:  ... loglikelihood=-6991.522303369757	0.9854655824075135
 91:  ... loglikelihood=-6949.932836393876	0.9855915702668652
 92:  ... loglikelihood=-6909.042944742059	0.9857519184514947
 93:  ... loglikelihood=-6868.833248982091	0.9858664528690871
 94:  ... loglikelihood=-6829.285113324573	0.9859351735196427
 95:  ... loglikelihood=-6790.380609275037	0.9860840682625129
 96:  ... loglikelihood=-6752.102481437123	0.9861756957965869
 97:  ... loglikelihood=-6714.4341153178075	0.9862673233306609
 98:  ... loglikelihood=-6677.359506996568	0.9864276715152903
 99:  ... loglikelihood=-6640.863234533149	0.9864849387240866
100:  ... loglikelihood=-6604.930430997365	0.9865651128164014
Writing tagger model to /Users/aihb/experiment/pos/my-pos/model4github/low-resource-pos-tagging-2014/2hours/t4/train4.nopipe.pipe.ser

 Re-training finished for train4!!!
ls ./2hours/t4/
ctest4
ctest4.nopipe
ctest4.nopipe.pipe
ctest4.nopipe.word
otest.nopipe.word
train4
train4.nopipe
train4.nopipe.pipe
train4.nopipe.pipe.ser
train4.nopipe.word
Training mode
Raw tokens: 108449  (5000 sentences)
Token-supervision tokens: 108449  (5000 sentences)
Type-supervision TD-entries: 10746  (10117 word types)
tsmooth: AddLambdaTransitionDistributioner(0.1)
esmooth: AddLambdaEmissionDistributioner(0.1)
emTrainer: SoftEmHmmTaggerTrainer(24, UnsmoothedTransitionDistributioner(), UnsmoothedEmissionDistributioner(), alphaT=0.000000, alphaE=0.000000)
Induce a soft tagging of the raw data
Extract a generalized tag dictionary
Induce a hard tagging via model minimization on the soft LP output
learn a smoothed HMM from EM
learn an HMM initialized with the estimated transition and emission distributions
raw tokens = 108449  (5000 sentences)
numWords = 10117
numTags  = 17

Make Indexed Distributions
Make Prior Counts (from the 5000 gold labeled sentences)
Start Training
iteration 1:   0.161 sec   avgLogProb=-141.29148924629857,   avgProb=4.343960629205954E-62
iteration 2:   0.132 sec   avgLogProb=-137.95732265101105,   avgProb=1.2186978207035567E-60
iteration 3:   0.134 sec   avgLogProb=-137.33391375134002,   avgProb=2.273207496712013E-60
iteration 4:   0.144 sec   avgLogProb=-137.19630778855986,   avgProb=2.608558555228506E-60
iteration 5:   0.126 sec   avgLogProb=-137.16446434809075,   avgProb=2.6929607302131036E-60
iteration 6:   0.127 sec   avgLogProb=-137.1525733115763,    avgProb=2.72517396942014E-60
iteration 7:   0.126 sec   avgLogProb=-137.14404461706638,   avgProb=2.7485155407714397E-60
iteration 8:   0.121 sec   avgLogProb=-137.1355893524552,    avgProb=2.7718534921953693E-60
iteration 9:   0.122 sec   avgLogProb=-137.12676144404185,   avgProb=2.7964314874880334E-60
iteration 10:  0.123 sec   avgLogProb=-137.11800937327385,   avgProb=2.8210134684707705E-60
iteration 11:  0.121 sec   avgLogProb=-137.1100358095774,    avgProb=2.84359691467336E-60
iteration 12:  0.125 sec   avgLogProb=-137.1033791894431,    avgProb=2.862588799903776E-60
iteration 13:  0.122 sec   avgLogProb=-137.09820729060104,   avgProb=2.877432170722958E-60
iteration 14:  0.121 sec   avgLogProb=-137.09437902122096,   avgProb=2.8884688684422014E-60
iteration 15:  0.120 sec   avgLogProb=-137.09162667934615,   avgProb=2.8964298729402847E-60
iteration 16:  0.141 sec   avgLogProb=-137.08968656044564,   avgProb=2.902054745977651E-60
iteration 17:  0.131 sec   avgLogProb=-137.08834460583833,   avgProb=2.905951785955235E-60
iteration 18:  0.132 sec   avgLogProb=-137.08743790847834,   avgProb=2.9085877996205576E-60
iteration 19:  0.132 sec   avgLogProb=-137.08684471243947,   avgProb=2.91031367422236E-60
iteration 20:  0.139 sec   avgLogProb=-137.0864745658086,    avgProb=2.9113911164171494E-60
iteration 21:  0.133 sec   avgLogProb=-137.08626063855414,   avgProb=2.912014008949824E-60
iteration 22:  0.128 sec   avgLogProb=-137.08615389798518,   avgProb=2.912324855571615E-60
iteration 23:  0.137 sec   avgLogProb=-137.08611867028168,   avgProb=2.9124274518952465E-60
iteration 24:  0.135 sec   avgLogProb=-137.08612925557313,   avgProb=2.912396623165011E-60
MAX ITERATIONS REACHED
Indexing events using cutoff of 10

	Computing event counts...  done. 108449 events
	Indexing...  done.
Sorting and merging events... done. Reduced 108449 events to 95909.
Done indexing.
Incorporating indexed data for training...  
done.
	Number of Event Tokens: 95909
	    Number of Outcomes: 15
	  Number of Predicates: 9700
...done.
Computing model parameters ...
Performing 100 iterations.
  1:  ... loglikelihood=-293685.33625936334	0.2716299827568719
  2:  ... loglikelihood=-128827.93634301373	0.8826545196359579
  3:  ... loglikelihood=-83564.17916661018	0.9056515043937703
  4:  ... loglikelihood=-63418.881937847786	0.9211149941447132
  5:  ... loglikelihood=-52037.20091053283	0.9313594408431614
  6:  ... loglikelihood=-44718.192381787354	0.9393539820560817
  7:  ... loglikelihood=-39598.101657600564	0.9452184897970475
  8:  ... loglikelihood=-35797.93658534672	0.9498658355540393
  9:  ... loglikelihood=-32852.35315575117	0.9529640660587004
 10:  ... loglikelihood=-30492.62666159951	0.9560715174874826
 11:  ... loglikelihood=-28552.756635244255	0.9589300039649974
 12:  ... loglikelihood=-26924.6432087597	0.9609217235751367
 13:  ... loglikelihood=-25534.834540985197	0.9625999317651615
 14:  ... loglikelihood=-24331.62692574964	0.9640291750039189
 15:  ... loglikelihood=-23277.52636866306	0.965273999760256
 16:  ... loglikelihood=-22344.65007689825	0.9662053130964785
 17:  ... loglikelihood=-21511.810604198752	0.9672288356739113
 18:  ... loglikelihood=-20762.602610205544	0.9682062536307389
 19:  ... loglikelihood=-20084.111347599523	0.9691652297393245
 20:  ... loglikelihood=-19466.01985565743	0.970068880303184
 21:  ... loglikelihood=-18899.978562965043	0.9706405775986869
 22:  ... loglikelihood=-18379.15140023729	0.971295263211279
 23:  ... loglikelihood=-17897.883089986117	0.9719683906721132
 24:  ... loglikelihood=-17451.451271922015	0.9725861925882212
 25:  ... loglikelihood=-17035.87911440508	0.973102564338998
 26:  ... loglikelihood=-16647.79176328084	0.9738217964204372
 27:  ... loglikelihood=-16284.305029671592	0.974292063550609
 28:  ... loglikelihood=-15942.938104963345	0.9748360980737489
 29:  ... loglikelihood=-15621.54440766236	0.9752141559627105
 30:  ... loglikelihood=-15318.25627625653	0.9756936440170034
 31:  ... loglikelihood=-15031.440354674593	0.9761270274506911
 32:  ... loglikelihood=-14759.661322741465	0.9765143062637738
 33:  ... loglikelihood=-14501.65220340096	0.9769569106215825
 34:  ... loglikelihood=-14256.289899072855	0.9773810731311492
 35:  ... loglikelihood=-14022.574917753676	0.9777314682477478
 36:  ... loglikelihood=-13799.61447787474	0.9781279679849515
 37:  ... loglikelihood=-13586.608352688021	0.9783861538603399
 38:  ... loglikelihood=-13382.836946501351	0.9788103163699066
 39:  ... loglikelihood=-13187.6511976574	0.9791514905623842
 40:  ... loglikelihood=-13000.463984177572	0.9793635718171675
 41:  ... loglikelihood=-12820.742772319261	0.979612536768435
 42:  ... loglikelihood=-12648.003299273023	0.9799168272644284
 43:  ... loglikelihood=-12481.804121438054	0.9802118968363009
 44:  ... loglikelihood=-12321.74189134443	0.9804147571669632
 45:  ... loglikelihood=-12167.4472511746	0.9806729430423518
 46:  ... loglikelihood=-12018.581250519846	0.980903466145377
 47:  ... loglikelihood=-11874.832211695357	0.9810694427795553
 48:  ... loglikelihood=-11735.91297858543	0.9812815240343388
 49:  ... loglikelihood=-11601.558495261294	0.9815397099097272
 50:  ... loglikelihood=-11471.52366905139	0.9816595819233004
 51:  ... loglikelihood=-11345.581479705514	0.9818993259504467
 52:  ... loglikelihood=-11223.521302083025	0.982037639812262
 53:  ... loglikelihood=-11105.147414619638	0.9822405001429243
 54:  ... loglikelihood=-10990.277669877903	0.9824710232459497
 55:  ... loglikelihood=-10878.742306869688	0.9827015463489751
 56:  ... loglikelihood=-10770.382887712203	0.9828214183625483
 57:  ... loglikelihood=-10665.051343585175	0.9830058368449686
 58:  ... loglikelihood=-10562.609117012891	0.9831625925550259
 59:  ... loglikelihood=-10462.926389221106	0.9833009064168411
 60:  ... loglikelihood=-10365.881382810736	0.9834668830510194
 61:  ... loglikelihood=-10271.359731248072	0.9836513015334397
 62:  ... loglikelihood=-10179.253907753688	0.983808057243497
 63:  ... loglikelihood=-10089.462707105928	0.9839924757259173
 64:  ... loglikelihood=-10001.890774669571	0.9841031268153695
 65:  ... loglikelihood=-9916.44817765781	0.9842783243736687
 66:  ... loglikelihood=-9833.050014223418	0.984435080083726
 67:  ... loglikelihood=-9751.616056507653	0.9846194985661463
 68:  ... loglikelihood=-9672.070424208294	0.9847578124279616
 69:  ... loglikelihood=-9594.341285637654	0.9848776844415348
 70:  ... loglikelihood=-9518.360583569854	0.9849791146068659
 71:  ... loglikelihood=-9444.063783487463	0.9851266493928021
 72:  ... loglikelihood=-9371.389642091075	0.9852649632546174
 73:  ... loglikelihood=-9300.279994173392	0.9854032771164326
 74:  ... loglikelihood=-9230.679556155601	0.9855139282058848
 75:  ... loglikelihood=-9162.535744768109	0.9856153583712159
 76:  ... loglikelihood=-9095.798509513326	0.9857721140812732
 77:  ... loglikelihood=-9030.420177684216	0.9858919860948464
 78:  ... loglikelihood=-8966.355310842971	0.9859749744119356
 79:  ... loglikelihood=-8903.560571769385	0.9861132882737508
 80:  ... loglikelihood=-8841.99460098242	0.9862147184390819
 81:  ... loglikelihood=-8781.61790203618	0.986279264907929
 82:  ... loglikelihood=-8722.392734855091	0.9864267996938653
 83:  ... loglikelihood=-8664.283016454305	0.9865651135556806
 84:  ... loglikelihood=-8607.254228443904	0.9867034274174957
 85:  ... loglikelihood=-8551.27333077607	0.986795636658706
 86:  ... loglikelihood=-8496.30868124232	0.9868878458999161
 87:  ... loglikelihood=-8442.329960271752	0.9870446016099733
 88:  ... loglikelihood=-8389.308100621676	0.9871829154717886
 89:  ... loglikelihood=-8337.215221588634	0.9872474619406357
 90:  ... loglikelihood=-8286.024567399096	0.9873027874853618
 91:  ... loglikelihood=-8235.710449469094	0.9873488921059669
 92:  ... loglikelihood=-8186.248192248413	0.9874226594989349
 93:  ... loglikelihood=-8137.614082388055	0.987477985043661
 94:  ... loglikelihood=-8089.785320994534	0.9875701942848713
 95:  ... loglikelihood=-8042.739978747839	0.9876439616778393
 96:  ... loglikelihood=-7996.45695368451	0.9877177290708075
 97:  ... loglikelihood=-7950.9159314612125	0.9877546127672915
 98:  ... loglikelihood=-7906.097347924421	0.9878929266291068
 99:  ... loglikelihood=-7861.982353834415	0.9879482521738329
100:  ... loglikelihood=-7818.552781594015	0.9879666940220749
Writing tagger model to /Users/aihb/experiment/pos/my-pos/model4github/low-resource-pos-tagging-2014/2hours/t5/train5.nopipe.pipe.ser

 Re-training finished for train5!!!
ls ./2hours/t5/
ctest5
ctest5.nopipe
ctest5.nopipe.pipe
ctest5.nopipe.word
otest.nopipe.word
train5
train5.nopipe
train5.nopipe.pipe
train5.nopipe.pipe.ser
train5.nopipe.word
Training mode
Raw tokens: 195881  (9000 sentences)
Token-supervision tokens: 195881  (9000 sentences)
Type-supervision TD-entries: 15072  (14177 word types)
tsmooth: AddLambdaTransitionDistributioner(0.1)
esmooth: AddLambdaEmissionDistributioner(0.1)
emTrainer: SoftEmHmmTaggerTrainer(8, UnsmoothedTransitionDistributioner(), UnsmoothedEmissionDistributioner(), alphaT=0.000000, alphaE=0.000000)
Induce a soft tagging of the raw data
Extract a generalized tag dictionary
Induce a hard tagging via model minimization on the soft LP output
learn a smoothed HMM from EM
learn an HMM initialized with the estimated transition and emission distributions
raw tokens = 195881  (9000 sentences)
numWords = 14177
numTags  = 17

Make Indexed Distributions
Make Prior Counts (from the 9000 gold labeled sentences)
Start Training
iteration 1:   0.368 sec   avgLogProb=-143.37698246647577,   avgProb=5.397189699856349E-63
iteration 2:   0.308 sec   avgLogProb=-139.78738888485924,   avgProb=1.9548271745746106E-61
iteration 3:   0.304 sec   avgLogProb=-138.81404180794266,   avgProb=5.1740143885533336E-61
iteration 4:   0.303 sec   avgLogProb=-138.61927889550017,   avgProb=6.286545892155097E-61
iteration 5:   0.304 sec   avgLogProb=-138.58068618399074,   avgProb=6.533903137220481E-61
iteration 6:   0.301 sec   avgLogProb=-138.57179120866442,   avgProb=6.592281295640952E-61
iteration 7:   0.303 sec   avgLogProb=-138.56985773436125,   avgProb=6.605039632108647E-61
iteration 8:   0.303 sec   avgLogProb=-138.56988373564147,   avgProb=6.60486789485502E-61
MAX ITERATIONS REACHED
Indexing events using cutoff of 10

	Computing event counts...  done. 195881 events
	Indexing...  done.
Sorting and merging events... done. Reduced 195881 events to 171686.
Done indexing.
Incorporating indexed data for training...  
done.
	Number of Event Tokens: 171686
	    Number of Outcomes: 15
	  Number of Predicates: 14264
...done.
Computing model parameters ...
Performing 100 iterations.
  1:  ... loglikelihood=-530455.5814434359	0.28674041892781843
  2:  ... loglikelihood=-225341.76097862673	0.8848688744697035
  3:  ... loglikelihood=-143843.00391164568	0.910833618370337
  4:  ... loglikelihood=-108012.27470442689	0.9260724623623526
  5:  ... loglikelihood=-88031.56094328122	0.9365124744104839
  6:  ... loglikelihood=-75313.48031217793	0.9437924045721637
  7:  ... loglikelihood=-66485.04618379546	0.9493110613076307
  8:  ... loglikelihood=-59972.55122861194	0.9532522296700547
  9:  ... loglikelihood=-54949.81626844559	0.9562999984684579
 10:  ... loglikelihood=-50942.85394768976	0.9587504658440584
 11:  ... loglikelihood=-47660.45522724194	0.9606495780601487
 12:  ... loglikelihood=-44913.86205613157	0.9624108514863616
 13:  ... loglikelihood=-42575.34739491509	0.9639474987364778
 14:  ... loglikelihood=-40555.357954743566	0.9652390992490338
 15:  ... loglikelihood=-38789.198150574506	0.9665102792001266
 16:  ... loglikelihood=-37228.91918854643	0.9675262021329276
 17:  ... loglikelihood=-35838.18950707872	0.9686033867501187
 18:  ... loglikelihood=-34588.94473982317	0.9694304194893839
 19:  ... loglikelihood=-33459.137729844	0.9704259218607215
 20:  ... loglikelihood=-32431.189222194214	0.9711304312312067
 21:  ... loglikelihood=-31490.89683183829	0.9718706765842526
 22:  ... loglikelihood=-30626.65109856455	0.972539449972177
 23:  ... loglikelihood=-29828.86191895677	0.9731265411142479
 24:  ... loglikelihood=-29089.53200235791	0.973739157958148
 25:  ... loglikelihood=-28401.934947523354	0.9743466696616824
 26:  ... loglikelihood=-27760.36900380433	0.9749184453826558
 27:  ... loglikelihood=-27159.966414397768	0.9753115411908251
 28:  ... loglikelihood=-26596.544143786898	0.9758118449466768
 29:  ... loglikelihood=-26066.485810024267	0.9762815178603336
 30:  ... loglikelihood=-25566.64742253914	0.9766541931070395
 31:  ... loglikelihood=-25094.281477393157	0.9770115529326479
 32:  ... loglikelihood=-24646.97534966327	0.9773127562142321
 33:  ... loglikelihood=-24222.600922267186	0.9776599057591089
 34:  ... loglikelihood=-23819.273119817826	0.9779508987599614
 35:  ... loglikelihood=-23435.315554597364	0.9782521020415457
 36:  ... loglikelihood=-23069.231893996683	0.9786196721478857
 37:  ... loglikelihood=-22719.68186219088	0.9789055600083725
 38:  ... loglikelihood=-22385.461019782873	0.9792220787110542
 39:  ... loglikelihood=-22065.483642148527	0.9795743333962967
 40:  ... loglikelihood=-21758.76815388781	0.9797836441512959
 41:  ... loglikelihood=-21464.424682913417	0.9800797422925144
 42:  ... loglikelihood=-21181.64438083257	0.9803401044511719
 43:  ... loglikelihood=-20909.69022185781	0.9805238895043419
 44:  ... loglikelihood=-20647.8890446911	0.9806923591364145
 45:  ... loglikelihood=-20395.62464374323	0.9809374058739745
 46:  ... loglikelihood=-20152.331749847046	0.9811926628922663
 47:  ... loglikelihood=-19917.490768080657	0.9813458171032412
 48:  ... loglikelihood=-19690.623162587395	0.9815193918756796
 49:  ... loglikelihood=-19471.28739644657	0.981723597490313
 50:  ... loglikelihood=-19259.075349485698	0.9818359105783614
 51:  ... loglikelihood=-19053.6091490654	0.982060536754458
 52:  ... loglikelihood=-18854.53835888153	0.9822494269479939
 53:  ... loglikelihood=-18661.537479112023	0.982402581158969
 54:  ... loglikelihood=-18474.303718097417	0.9825199993873831
 55:  ... loglikelihood=-18292.555001517405	0.9826680484579924
 56:  ... loglikelihood=-18116.028189835855	0.9828977797744549
 57:  ... loglikelihood=-17944.47747886116	0.9830407237046983
 58:  ... loglikelihood=-17777.67296169119	0.9832142984771366
 59:  ... loglikelihood=-17615.39933324197	0.9833317167055509
 60:  ... loglikelihood=-17457.45472102271	0.9834542400743308
 61:  ... loglikelihood=-17303.649627943814	0.9835869737238425
 62:  ... loglikelihood=-17153.805974744122	0.9837503382155492
 63:  ... loglikelihood=-17007.75623118723	0.9838728615843293
 64:  ... loglikelihood=-16865.342626505197	0.9839902798127435
 65:  ... loglikelihood=-16726.41643071757	0.9841230134622552
 66:  ... loglikelihood=-16590.837299467956	0.9842455368310352
 67:  ... loglikelihood=-16458.472675859586	0.9843629550594494
 68:  ... loglikelihood=-16329.197243555229	0.9845467401126194
 69:  ... loglikelihood=-16202.892426031302	0.984715209744692
 70:  ... loglikelihood=-16079.445927475672	0.984807102271277
 71:  ... loglikelihood=-15958.751311300155	0.9849194153593253
 72:  ... loglikelihood=-15840.70761269463	0.9850623592895686
 73:  ... loglikelihood=-15725.218982017619	0.9852001980794461
 74:  ... loglikelihood=-15612.194356182026	0.9853074060271287
 75:  ... loglikelihood=-15501.547155468412	0.9853941934133479
 76:  ... loglikelihood=-15393.195003483592	0.985480980799567
 77:  ... loglikelihood=-15287.059468206618	0.9855728733261521
 78:  ... loglikelihood=-15183.0658222727	0.9856443452912738
 79:  ... loglikelihood=-15081.142820829025	0.985731132677493
 80:  ... loglikelihood=-14981.222495470873	0.9858077097829805
 81:  ... loglikelihood=-14883.239962888072	0.9858893920288339
 82:  ... loglikelihood=-14787.133247011603	0.9859812845554189
 83:  ... loglikelihood=-14692.84311353971	0.9860884925031014
 84:  ... loglikelihood=-14600.312915847227	0.9861701747489547
 85:  ... loglikelihood=-14509.48845135587	0.9862671724159056
 86:  ... loglikelihood=-14420.317827547242	0.9863641700828564
 87:  ... loglikelihood=-14332.751336851821	0.986440747188344
 88:  ... loglikelihood=-14246.74133973796	0.9865122191534657
 89:  ... loglikelihood=-14162.242155363288	0.986624532241514
 90:  ... loglikelihood=-14079.20995922614	0.9867113196277332
 91:  ... loglikelihood=-13997.60268728499	0.9868032121543182
 92:  ... loglikelihood=-13917.379946074325	0.9869104201020007
 93:  ... loglikelihood=-13838.502928372734	0.9870125229093174
 94:  ... loglikelihood=-13760.934334026628	0.9871095205762682
 95:  ... loglikelihood=-13684.6382955564	0.9871656771202925
 96:  ... loglikelihood=-13609.580308208857	0.9872014131028533
 97:  ... loglikelihood=-13535.727164143771	0.9872626747872433
 98:  ... loglikelihood=-13463.046890466348	0.987334146752365
 99:  ... loglikelihood=-13391.508690846442	0.9874617752615108
100:  ... loglikelihood=-13321.082890473626	0.9875281420862667
Writing tagger model to /Users/aihb/experiment/pos/my-pos/model4github/low-resource-pos-tagging-2014/2hours/t9/train9.nopipe.pipe.ser

 Re-training finished for train9!!!
ls ./2hours/t9/
ctest9
ctest9.nopipe
ctest9.nopipe.pipe
ctest9.nopipe.word
otest.nopipe.word
train9
train9.nopipe
train9.nopipe.pipe
train9.nopipe.pipe.ser
train9.nopipe.word
